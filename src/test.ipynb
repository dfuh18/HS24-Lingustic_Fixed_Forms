{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Test the knowledge of LLMs of common idioms (list form https://englishlovers.in/what-is-idiom-examples-definition-list-of-1000-idiom/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[notice] A new release of pip is available: 24.0 -> 24.3.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\gian-\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "Note: you may need to restart the kernel to use updated packages.  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)"
     ]
    }
   ],
   "source": [
    "# %pip install -U \"ell-ai[all]\"\n",
    "# %pip install pandas\n",
    "# %pip install python-dotenv\n",
    "# %pip install ollama\n",
    "#%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports and setup\n",
    "import pandas as pd\n",
    "import ell\n",
    "from pydantic import BaseModel, Field\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "# import openai\n",
    "import ollama\n",
    "from sklearn.metrics.pairwise import cosine_similari\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             idiom  \\\n",
      "0                       A Bit Much   \n",
      "1             A Bite at The Cherry   \n",
      "2                       A Busy Bee   \n",
      "3             A Cat Has Nine Lives   \n",
      "4  A Cat in Gloves Catches No Mice   \n",
      "\n",
      "                                             meaning  \n",
      "0            More than is reasonable; a bit too much  \n",
      "1   A good opportunity that isn’t available to ev...  \n",
      "2   A busy, active person who moves quickly from ...  \n",
      "3        Cats seem to get away with dangerous things  \n",
      "4   You can’t get what you need if you’re too car...  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450, 2)\n"
     ]
    }
   ],
   "source": [
    "# read in the prepared csv file\n",
    "df = pd.read_csv('../data/idioms.csv', delimiter=':', skiprows=1, names=['idiom', 'meaning'])\t\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare function for idiom translation using ell with specified output format \n",
    "\n",
    "prompt = \"Translate the following idiom, or phrase, into the most concise and accurate translation possible only include a single answer:\"\n",
    "\n",
    "\n",
    "# model with OpenAI that type safety / structured outputs needs api key and funding\n",
    "\n",
    "# class IdiomTranslation(BaseModel):\n",
    "#     generatedTranslation: str = Field(description=\"The translation of the presented idiom as concise and accurate as possible\")\n",
    "\n",
    "# @ell.complex(model=\"gpt-4o-mini\", response_format=IdiomTranslation, temperature=0)\n",
    "# def translate_idiom(idiom: str) -> IdiomTranslation:\n",
    "#     return [ell.system(prompt), ell.user(idiom)]\n",
    "\n",
    "# run with local ollama\n",
    "# Use models automatically registered by asking ollama\n",
    "ell.models.ollama.register(base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "ell.init(store='./logdir', autocommit=True, autocommit_model=\"llama3.1:8b\", verbose=True)\n",
    "\n",
    "\n",
    "@ell.complex(model=\"llama3.1:8b\", temperature=0)  # adding response format leads to validation errors\n",
    "def translate_idiom(idiom: str):\n",
    "    return [ell.system(prompt), ell.user(idiom)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idiom: A Bit Much\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║ translate_idiom(A Bit Mu..)\n",
      "╠══════════════════════════════════════════════════════════════════════════════╣\n",
      "║ Prompt:\n",
      "╟──────────────────────────────────────────────────────────────────────────────╢\n",
      "│      system: Translate the following idiom, or phrase, into the most\n",
      "│              concise and accurate translation possible only include a\n",
      "│              single answer:\n",
      "│\n",
      "│        user: A Bit Much\n",
      "╟──────────────────────────────────────────────────────────────────────────────╢\n",
      "║ Output:\n",
      "╟──────────────────────────────────────────────────────────────────────────────╢\n",
      "│   assistant: Too much to handle.\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "Translation: Too much to handle.\n",
      "Answer form the csv:  More than is reasonable; a bit too much\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "idiom = df['idiom'][0]\n",
    "print(f\"Idiom: {idiom}\")\n",
    "\n",
    "response = translate_idiom(idiom)\n",
    "print(f\"Translation: {response.content[0].text}\")\n",
    "\n",
    "print(f\"Answer form the csv: {df['meaning'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.7029551047916202\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Translation of the model using similarity score of embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedding_translation = ollama.embeddings(model=\"nomic-embed-text\", prompt=response.content[0].text)\n",
    "embedding_real_answer = ollama.embeddings(model=\"nomic-embed-text\", prompt=df['meaning'][0])\n",
    "\n",
    "# Reshape the embeddings to 2D arrays\n",
    "embedding_translation_reshaped = [embedding_translation.embedding]\n",
    "embedding_real_answer_reshaped = [embedding_real_answer.embedding]\n",
    "\n",
    "similarity_score = cosine_similarity(embedding_translation_reshaped, embedding_real_answer_reshaped)\n",
    "print(f\"Similarity score: {similarity_score[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate a model (translate all idioms and compare to the real answers for a given model)\n",
    "def evaluate_model(df, model):\n",
    "    scores = []\n",
    "    # counter = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        # counter += 1\n",
    "        idiom = df['idiom'][i]\n",
    "        response = model(idiom)\n",
    "        embedding_translation = ollama.embeddings(model=\"nomic-embed-text\", prompt=response.content[0].text)\n",
    "        embedding_real_answer = ollama.embeddings(model=\"nomic-embed-text\", prompt=df['meaning'][i])\n",
    "        embedding_translation_reshaped = [embedding_translation.embedding]\n",
    "        embedding_real_answer_reshaped = [embedding_real_answer.embedding]\n",
    "        similarity_score = cosine_similarity(embedding_translation_reshaped, embedding_real_answer_reshaped)\n",
    "        scores.append(similarity_score[0][0])\n",
    "        # if counter == 20:\n",
    "        #     break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.7029551047916202), np.float64(0.6301009567222098), np.float64(0.5834982262759125), np.float64(0.42137171566679565), np.float64(0.5331034253454627), np.float64(0.7661584280519161), np.float64(0.5916024939260345), np.float64(0.7214854138672798), np.float64(0.532905051594004), np.float64(0.6488030518668693), np.float64(0.6209738865324419), np.float64(0.7677581845446777), np.float64(0.4325530505028463), np.float64(0.4366071028157236), np.float64(0.5909186913806793), np.float64(0.6765920221895259), np.float64(0.48033139946408177), np.float64(0.7600456465792751), np.float64(0.5203125677867839), np.float64(0.6635981714647708)]\n"
     ]
    }
   ],
   "source": [
    "# test evaluate_model\n",
    "ell.init(verbose=False)\n",
    "scores = evaluate_model(df, translate_idiom)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a csv file\n",
    "def write_translations_to_csv(df, model, filename='translations.csv'):\n",
    "    header = ['idiom', 'meaning', 'model', 'response']\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        for i in range(df.shape[0]):\n",
    "            idiom = df['idiom'][i]\n",
    "            response = model(idiom)\n",
    "            writer.writerow([idiom, df['meaning'][i], 'llama3.1:8b', response.content[0].text])\n",
    "\n",
    "def calculate_similarity_scores_and_write_to_csv(df, filename='translations_with_similarity_scores.csv'):\n",
    "    header = ['idiom', 'meaning', 'response', 'similarity_score']\n",
    "    df_translations = pd.read_csv('translations.csv', skiprows=1, names=['idiom', 'meaning','model', 'response'])\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        for i in range(df.shape[0]):\n",
    "            idiom = df_translations.iloc[i]['idiom']\n",
    "            response = df_translations.iloc[i]['response']\n",
    "            embedding_translation = ollama.embeddings(model=\"nomic-embed-text\", prompt=response)\n",
    "            embedding_real_answer = ollama.embeddings(model=\"nomic-embed-text\", prompt=df['meaning'][i])\n",
    "            embedding_translation_reshaped = [embedding_translation.embedding]\n",
    "            embedding_real_answer_reshaped = [embedding_real_answer.embedding]\n",
    "            similarity_score = cosine_similarity(embedding_translation_reshaped, embedding_real_answer_reshaped)\n",
    "            writer.writerow([idiom, df['meaning'][i], response, similarity_score[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test eval_and_write\n",
    "# write_translations_to_csv(df, translate_idiom, 'translations.csv')\n",
    "calculate_similarity_scores_and_write_to_csv(df, 'translations_with_similarity_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1450.000000\n",
      "mean        0.660420\n",
      "std         0.132097\n",
      "min         0.319372\n",
      "25%         0.565719\n",
      "50%         0.657454\n",
      "75%         0.753543\n",
      "max         1.000000\n",
      "Name: similarity_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# see basic statistics of the similarity scores\n",
    "df_similarity_scores = pd.read_csv('translations_with_similarity_scores.csv', skiprows=1, names=['idiom', 'meaning', 'response', 'similarity_score'])\n",
    "print(df_similarity_scores['similarity_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 idiom  \\\n",
      "17         A Home Bird   \n",
      "55     Ace In The Hole   \n",
      "98   All Hands on Deck   \n",
      "125         All Thumbs   \n",
      "127            All Wet   \n",
      "\n",
      "                                                              meaning  \\\n",
      "17    Somebody who prefers to spend his social and free time at home.   \n",
      "55                                                 A hidden advantage   \n",
      "98                                                Everyone must help.   \n",
      "125                                                            Clumsy   \n",
      "127                                               Completely mistaken   \n",
      "\n",
      "                                                     response  \\\n",
      "17   A person who prefers to stay at home rather than go out.   \n",
      "55                                        A hidden advantage.   \n",
      "98                               \"Everyone's help is needed.\"   \n",
      "125                                                   Clumsy.   \n",
      "127                             Completely mistaken or wrong.   \n",
      "\n",
      "     similarity_score  \n",
      "17           0.900848  \n",
      "55           0.988062  \n",
      "98           0.903481  \n",
      "125          0.979513  \n",
      "127          0.912698  \n"
     ]
    }
   ],
   "source": [
    "# have a look at some entries with very high similarity scores \n",
    "# change display settings to show the full text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df_similarity_scores[df_similarity_scores['similarity_score'] > 0.9].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   idiom  \\\n",
      "56   Ace Up One�s Sleeve   \n",
      "103     All in One Piece   \n",
      "131         Amber Nectar   \n",
      "182           Baby Blues   \n",
      "374  Come Rain and Shine   \n",
      "\n",
      "                                                  meaning  \\\n",
      "56    A surprise advantage of which others are not aware.   \n",
      "103                                                Safely   \n",
      "131                                                  Beer   \n",
      "182                                            Blue eyes.   \n",
      "374              Do regularly, whatever the circumstances   \n",
      "\n",
      "                    response  similarity_score  \n",
      "56             A trump card.          0.396890  \n",
      "103            \"Whole cloth\"          0.390095  \n",
      "131            Honey of Gold          0.393247  \n",
      "182   Postpartum depression.          0.319372  \n",
      "374  Through thick and thin.          0.393893  \n"
     ]
    }
   ],
   "source": [
    "# have a look at some entries with very low similarity scores\n",
    "print(df_similarity_scores[df_similarity_scores['similarity_score'] < 0.4].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
